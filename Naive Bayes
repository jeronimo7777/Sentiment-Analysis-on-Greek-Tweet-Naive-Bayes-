import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import re # for regex

# NLTK library
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize



# Bag of Words
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB, CategoricalNB
from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix


# Remove stop words
from stopwordsiso import stopwords

# stemming in greek
from snowballstemmer import stemmer

# create plots
import matplotlib.pyplot as plt


#####-------------------------Functions---------------------------#####
# We clean up any HTML element that may lurk in our data
def clean(text):
    cleaned = re.compile(r'<.*?>')
    return re.sub(cleaned, '', text)


# Remove words that are contain symbols that are not
# letters or numbers.
def is_special1(text):
    rem = ''
    for i in text:
        if i.isalnum() == False and i != " ":
            text = text.replace(i, ' ')
    return text

# Remove numbers
def remove_numbers(text):
    text_without_numbers = re.sub(r'\d', '', text)
    return text_without_numbers

# Transform text to lower case
def to_lower(text):
    text = text.lower()
    return text

def remove_acute_accents(text):
    text = text.replace("ά", "α")
    text = text.replace("έ", "ε")
    text = text.replace("ή", "η")
    text = text.replace("ί", "ι")
    text = text.replace("ό", "ο")
    text = text.replace("ύ", "υ")
    text = text.replace("ώ", "ω")
    return text

# Remove words with one or two letters length
def remove_low_letter_words(text):
    words = text.split()  # Split text into words
    filtered_words = [word for word in words if len(word) > 2]  # Filter out one-letter words
    processed_text = ' '.join(filtered_words)  # Join the filtered words back into a string
    return processed_text



# Remove stopwords (i.e. words like the, as, if, that etc)
def rem_stopwords(text):
    stop_words = set(stopwords('el'))
    words = word_tokenize(text)
    return " ".join([w for w in words if w not in stop_words])


# Remove RT,@,https for retweet
def remove_rt(text):
    return ' '.join([word for word in text.split() if not (word.startswith('RT') or word.startswith('@') or word.startswith('https'))])

# Performing text stemming
def stem_txt(text):
    words = word_tokenize(text)
    ss = stemmer('greek')
    return " ".join([ss.stemWord(w) for w in words])

#####---------------------------------MAIN CODE-----------------------------------------------#####



print("\n\n********************************\n** Naive Bayes Classification **\n********************************\n")


#####----- Importing CSV file -----#####
##### The tweet text must be in a column named "tweet" and the sentiment on a column named "sentiment".
##### We assumed that we will import a csv file
print('Importing CSV files')
all_tweets = pd.read_csv('YOUR CSV FILE.csv', delimiter=';', usecols=['tweet', 'sentiment'])
print('done')


#####----- Pre-processing -----#####

print("\tLower case....", end="")
all_tweets.tweet = all_tweets.tweet.apply(to_lower)
print("done")

print("\tRemove acute accents", end="")
all_tweets.tweet = all_tweets.tweet.apply(remove_acute_accents)
print("done")

print("\tCleaning html entities....", end="")
all_tweets.tweet = all_tweets.tweet.apply(clean)
print("done")

print("\tRemove rt and names from users......", end="")
all_tweets.tweet = all_tweets.tweet.apply(remove_rt)
print( "done" )


print("\tRemoving special characters....", end="")
all_tweets.tweet = all_tweets.tweet.apply(is_special1)
print("done")


print("\tRemoving numbers....", end="")
all_tweets.tweet = all_tweets.tweet.apply(remove_numbers)
print("done")


print("\tRemoving stopwords....", end="")
all_tweets.tweet = all_tweets.tweet.apply(rem_stopwords)
print( "done" )


print("\tStemming....", end="")
all_tweets.tweet = all_tweets.tweet.apply(stem_txt)
print( "done" )


#####----- Count Vectorize -----#####
print("\tText Vectorization....", end="")
y = all_tweets.sentiment.values
cv = CountVectorizer(max_features = 2000)
X= cv.fit_transform(all_tweets.tweet).toarray()
cv_length = X.shape[1]
print("done")

# Perform sentiment analysis on each piece
NBModel = MultinomialNB(alpha=1.0, fit_prior=True)

print("Training multinomial model....", end="")
trainx,testx,trainy,testy = train_test_split(X,y,test_size=0.2,random_state=9)
NBModel.fit(trainx,trainy)
print("done.")

print("Predicting categories on the testing set.")

# Use the testing set to do some predictions.
ypm = NBModel.predict(testx)

# Calculate some metrics: accuracy, precision and recall for
# the testing set
print("\tMultinomial accuracy = ",accuracy_score(testy,ypm))
print("\tMultinomial precision= ",precision_score(testy,ypm, average=None))
print("\tMultinomial recall= ",recall_score(testy,ypm, average=None))

cm = confusion_matrix(testy,ypm)

print("Confusion Matrix:")
print(cm)

print("Predicting categories on unknown data.")

#####----- Import unknown data -----#####
print("Import unknown data...", end="")
csv_files = [ 'UNKNOWN DATA.csv']
unknown_tweets = []
for file in csv_files:
    df = pd.read_csv(file, delimiter=';', usecols=['tweet'])
    unknown_tweets.extend(df['tweet'].tolist())
unknown_tweets = pd.DataFrame({'tweet': unknown_tweets}, columns=['tweet'])
print('done')

print("Shuffle randomly the unknown tweets...", end="")
unknown_tweets = unknown_tweets.sample(frac=1, random_state=42).reset_index(drop=True)
print('done')

#####----- Pre-processing of Unknown Data -----####

print("Pre-processing of unkown data")

print("\tCleaning html entities....", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(clean)
print("done")

print("\tRemove rt and names from users......", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(remove_rt)
print( "done" )

print("\tRemoving special characters....", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(is_special1)
print("done")

print("\tRemoving numbers....", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(remove_numbers)
print("done")

print("\tLower case....", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(to_lower)
print("done")

print("\tRemove acute accents", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(remove_acute_accents)
print("done")

print("\tRemoving stopwords....", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(rem_stopwords)
print( "done" )

print("\tStemming....", end="")
unknown_tweets.tweet = unknown_tweets.tweet.apply(stem_txt)
print( "done" )

#

#####----- Count Vectorize -----#####
print("\tText Vectorization....", end="")
cv = CountVectorizer(max_features=cv_length)
vectorized_tweets = cv.fit_transform(unknown_tweets.tweet).toarray()
print("done")



# Perform sentiment analysis on each piece
sentiments = []
NBModel.fit(trainx, trainy)
predicted_sentiment = NBModel.predict(vectorized_tweets)
sentiments.extend(predicted_sentiment)


# Plot the sentiment distribution for all the unknown tweets
sentiment_counts = pd.Series(sentiments).value_counts()
sentiment_labels = sentiment_counts.index
sentiment_values = sentiment_counts.values

# Plot the sentiment distribution for all the unknown tweets
sentiment_labels = ['positive' if label == 1 else 'negative' for label in sentiment_labels]

plt.bar(sentiment_labels, sentiment_values, width=0.15,  linewidth=0.5)
plt.xlabel('Sentiment')
plt.ylabel('Number of Tweets')
plt.show()

# Create a DataFrame to count sentiment occurrences
sentiment_counts = pd.Series(sentiments).value_counts()
sentiment_table = pd.DataFrame({
    'Sentiment': sentiment_counts.index,
    'Count': sentiment_counts.values
})

# Print the sentiment table
print(sentiment_table)

